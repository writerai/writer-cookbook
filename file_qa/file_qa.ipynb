{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Q&A\n",
    "This notebook will show you how to create an application that can answer your questions based on information in a group of files. This can be useful if you have hundreds of pages of information that you don't want to spend the time searching through yourself. Since it uses semantic information matching, there's no need to use the exact same wording that your documentation might use.\n",
    "\n",
    "At a high level, this application will:\n",
    "* Split your source material into relatively small chunks of text\n",
    "* Assign [embeddings](https://txt.cohere.com/text-embeddings/) to each chunk\n",
    "* Compare your query to each chunk and assign a relevance score\n",
    "* Give your query along with the most relevant chunks of info to a Writer model to summarize it into an understandable answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from writer import Writer\n",
    "from writer.models import operations, shared\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to give the library our Writer API info. Make sure you have a file named `.env` in the parent directory with the lines \n",
    "```\n",
    "WRITER_ORG_ID=YOUR_ORG_ID\n",
    "WRITER_API_KEY=YOUR_API_KEY\n",
    "```\n",
    "or simply set the variables in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"..\")\n",
    "org_id = os.environ.get(\"WRITER_ORG_ID\")\n",
    "api_key = os.environ.get(\"WRITER_API_KEY\")\n",
    "\n",
    "writer = Writer(\n",
    "    security=shared.Security(\n",
    "        api_key=api_key\n",
    "    ),\n",
    "    organization_id=org_id\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we need a function to extract text from a file. This example only handles PDFs, but can be easily extended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(dir: str, filename: str):\n",
    "    file = open(f\"{dir}/{filename}\", \"rb\")\n",
    "    _, ext = os.path.splitext(filename)\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a function to split the extracted text into chunks. For now a chunk size of 50 should work, but tweaking it could yield better results depending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_TOKENS = 50\n",
    "\n",
    "def chunk_text(text: str):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    # We can tokenize the text to count the number of tokens for chunking\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    chunks = [\n",
    "        tokenizer.decode(tokens[i : i + CHUNK_TOKENS])\n",
    "        for i in range(0, len(tokens), CHUNK_TOKENS)\n",
    "    ]\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to determine which chunks of text are actually relevant to the question. To do this, we use `all-MiniLM-L6-v2` (Found on huggingface [here](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)), which has sentence similarity functionality. It's a relatively lightweight model to run locally, but the direct calls can be replaced with API calls if you wish. Alternatively, other models with similar abilities could be used instead.\n",
    "\n",
    "In this function, TOP_N is how many chunks of text it will return. A higher number means more context, but it could just be extra irrelevant information, so a balance is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 10\n",
    "\n",
    "def get_relevant_text(query: str, chunks: list[str]):\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    text_embeddings = model.encode(chunks)\n",
    "    query_embeddings = model.encode(query)\n",
    "\n",
    "    cos_sims = util.cos_sim(query_embeddings, text_embeddings)\n",
    "\n",
    "    sorted_chunk_nums = sorted(zip(cos_sims.tolist()[0], range(len(chunks))))\n",
    "    # We also take the chunk before and after each relevant chunk. \n",
    "    # Since the text is split arbitrarily, this is just in case some important context got cut off.\n",
    "    relevant_text = \"\\n\".join(\n",
    "        chunks[i - 1] or \"\" + chunks[i] + chunks[i + 1] or \"\"\n",
    "        for _, i in sorted_chunk_nums[-TOP_N:]\n",
    "    )\n",
    "\n",
    "    return relevant_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will send your question and relevant chunks along with an instructional prompt to Writer's `palmyra-instruct` model for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query: str, text: str):\n",
    "    prompt = (\n",
    "        f\"Given the following context information, try to answer the following question with as much detail as possible.\"\n",
    "        f\"Use only the given context. Do not use outside information.\\n\"\n",
    "        f\"If no answer can be found in the given context, output \\\"Could not find an answer in your files.\\\"\\n\"\n",
    "        f\"Question: {query} \\n\\nContext:\\n{text}\\nAnswer: \"\n",
    "    )\n",
    "\n",
    "    req = operations.CreateCompletionRequest(\n",
    "        completion_request=shared.CompletionRequest(\n",
    "            prompt=prompt, max_tokens=2000, temperature=1.1\n",
    "        ),\n",
    "        model_id=\"palmyra-instruct\",\n",
    "    )\n",
    "\n",
    "    res = writer.completions.create(req)\n",
    "    if res.completion_response is not None:\n",
    "        return res.completion_response.choices[0].text\n",
    "    else:\n",
    "        print(res.fail_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a function to combine everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR = \"files\"\n",
    "\n",
    "def run_query(query: str):\n",
    "    context = \"\"\n",
    "    for filename in os.listdir(FILE_DIR):\n",
    "        context += extract_text_from_file(FILE_DIR, filename)\n",
    "\n",
    "    chunks = chunk_text(context)\n",
    "\n",
    "    relevant_text = get_relevant_text(query, chunks)\n",
    "\n",
    "    answer = get_answer(query, relevant_text)\n",
    "    return answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask it a question! For this example, I put an old Macbook Pro user manual in the `files` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To add a wifi network step by step, you should:\n",
      "1. Choose Apple ( ï£¿) > System Preferences and click Network. \n",
      "2. Select Wifi in the list of the interfaces and click Create Network. \n",
      "3. Provide a Network Name and optionally a Password, select the type of encryption you wish to use and press OK.\n",
      "4. Make sure the Airport is ON and click Advanced. \n",
      "5. Configure any settings and click OK to save the changes and reconnect to the wifi network.\n"
     ]
    }
   ],
   "source": [
    "answer = run_query(\"How do I add a wifi network step by step?\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
