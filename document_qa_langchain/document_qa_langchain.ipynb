{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Q&A using Langchain\n",
    "\n",
    "This notebook will go over how to use Langchain's Writer integration to answer questions based on the contents of a set of documents. It will accomplish roughly the same result as the File Q&A example in this repository, but with the added simplicity of using built in Langchain functionality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "Make sure you have a virtual environment selected if you don't want to install these globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --disable-pip-version-check\\\n",
    "    langchain chromadb setuptools tiktoken python-dotenv pypdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Writer Langchain integration looks for two environment variables: `WRITER_ORG_ID` and `WRITER_API_KEY`. If you're running this notebook, make sure you have a `.env` file in the parent directory that looks like this:\n",
    "```\n",
    "WRITER_ORG_ID=<your org ID>\n",
    "WRITER_API_KEY=<your API key>\n",
    "```\n",
    "and run the following cell. You can alternatively use whatever method you want to set environment variables as long as they get set. \n",
    "\n",
    "Yet another alternative if you don't want environment variables is passing these parameters to the Langchain `Writer` object on construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Writer\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting document data\n",
    "\n",
    "The first step is to extract all of the written information from our documents. This example only handles pdfs, but it can be easily expanded to handle plaintext, markdown, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "\n",
    "def extract_text_from_file(dir: str, filename: str):\n",
    "    file = open(f\"{dir}/{filename}\", \"rb\")\n",
    "    _, ext = os.path.splitext(filename)\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to split up our text into chunks and find the most relevant ones. In the File Q&A example we did this manually, but Langchain has built in functionality to make this easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "text_splitter = TokenTextSplitter(encoding_name=\"cl100k_base\", chunk_size=150, chunk_overlap=50)\n",
    "embeddings = SentenceTransformerEmbeddings()\n",
    "\n",
    "def get_relevant_chunks(text: str, query: str):\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    docsearch = Chroma.from_texts(chunks, embeddings).as_retriever()\n",
    "\n",
    "    relevant_chunks = docsearch.get_relevant_documents(query)\n",
    "    return relevant_chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting an answer\n",
    "\n",
    "Now all we need to do is feed our query and the relevant chunks to a Writer model. We do this by first creating a qa chain of type \"stuff\" and giving it a Writer instance, then running that chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'\\n%PDF'\n"
     ]
    }
   ],
   "source": [
    "FILE_DIR = \"documents\"\n",
    "org_id = os.environ.get(\"WRITER_ORG_ID\")\n",
    "model_id = 'palmyra-instruct'\n",
    "\n",
    "chain = load_qa_chain(\n",
    "    Writer(\n",
    "        max_tokens=500,\n",
    "        temperature=0\n",
    "    ), \n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "context = \"\"\n",
    "for filename in os.listdir(FILE_DIR):\n",
    "    context += extract_text_from_file(FILE_DIR, filename)\n",
    "\n",
    "def run_query(query: str):\n",
    "    relevant_chunks = get_relevant_chunks(context, query)\n",
    "\n",
    "    answer = chain.run(input_documents=relevant_chunks, question=query)\n",
    "    return json.loads(answer)[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then just ask it a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Choose \"Turn AirPort on\" from the AirPort (Z) status menu in the menu bar. AirPort will then detect available wireless networks.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_query(\"How do I enable wifi?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
