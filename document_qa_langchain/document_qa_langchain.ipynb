{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Q&A using Langchain\n",
    "\n",
    "This notebook will go over how to use Langchain's Writer integration to answer questions based on the contents of a set of documents. It will accomplish roughly the same result as the File Q&A example in this repository, but with the added simplicity of using built in Langchain functionality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "Make sure you have a virtual environment selected if you don't want to install these globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain chromadb tiktoken python-dotenv pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Writer\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting document data\n",
    "\n",
    "The first step is to extract all of the written information from our documents. This example only handles pdfs, but it can be easily expanded to handle plaintext, markdown, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "\n",
    "def extract_text_from_file(dir: str, filename: str):\n",
    "    file = open(f\"{dir}/{filename}\", \"rb\")\n",
    "    _, ext = os.path.splitext(filename)\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to split up our text into chunks and find the most relevant ones. In the File Q&A example we did this manually, but Langchain has built in functionality to make this easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(encoding_name=\"cl100k_base\", chunk_size=150, chunk_overlap=50)\n",
    "embeddings = SentenceTransformerEmbeddings()\n",
    "\n",
    "def get_relevant_chunks(text: str, query: str):\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    docsearch = Chroma.from_texts(chunks, embeddings).as_retriever()\n",
    "\n",
    "    relevant_chunks = docsearch.get_relevant_documents(query)\n",
    "    return relevant_chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting an answer\n",
    "\n",
    "Now all we need to do is feed our query and the relevant chunks to a Writer model. We do this by first creating a qa chain of type \"stuff\" and giving it a Writer instance, then running that chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR = \"documents\"\n",
    "load_dotenv()\n",
    "org_id = os.environ.get(\"WRITER_ORG_ID\")\n",
    "model_id = 'palmyra-instruct'\n",
    "\n",
    "chain = load_qa_chain(\n",
    "    Writer(\n",
    "        base_url=f\"https://enterprise-api.writer.com/llm/organization/{org_id}/model/{model_id}/completions\", \n",
    "        tokens_to_generate=500\n",
    "    ), \n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "def run_query(query: str):\n",
    "    context = \"\"\n",
    "    for filename in os.listdir(FILE_DIR):\n",
    "        context += extract_text_from_file(FILE_DIR, filename)\n",
    "\n",
    "    relevant_chunks = get_relevant_chunks(context, query)\n",
    "\n",
    "    answer = chain.run(input_documents=relevant_chunks, question=query)\n",
    "    return json.loads(answer)[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then just ask it a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'\\n%PDF'\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Choose \"Turn AirPort on\" from the AirPort (Z) status menu'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_query(\"How do I enable wifi?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
