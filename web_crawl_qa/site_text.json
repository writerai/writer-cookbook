[
{"url": "https://dev.writer.com/docs/quickstart", "text": "Integrating Writer into your app or website can begin as soon as you create a Writer developer account, requiring only three steps:\nWriter authenticates your API requests using your account\u2019s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their organization as an org admin.\nYou can run a test directly from the API interface or you can run your first API request into your terminal.\nUpdated\n \nabout 2 months ago"},
{"url": "https://dev.writer.com/recipes", "text": ""},
{"url": "https://dev.writer.com/edit/quickstart", "text": "Integrating Writer into your app or website can begin as soon as you create a Writer developer account, requiring only three steps:\nObtain your API keys so Writer can authenticate your integration\u2019s API requests\nMake an API request to confirm everything is up and running \nWriter authenticates your API requests using your account\u2019s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\n\ufeff\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their organization as an org admin.\nYou can run a test directly from the API interface or you can run your first API request into your terminal.\nAuthorization\nVariable\nhttpAuth\nVariable\n-1\n+1\n100\n1234\n8ball\na\nab\nabc\nabcd\naccept\naerial-tramway\nairplane\nalarm-clock\nalien\nambulance\nanchor\nangel\nanger\nangry\nanguished\nant\napple\naquarius\naries\narrow-backward"},
{"url": "https://dev.writer.com/docs", "text": "Integrating Writer into your app or website can begin as soon as you create a Writer developer account, requiring only three steps:\nWriter authenticates your API requests using your account\u2019s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their organization as an org admin.\nYou can run a test directly from the API interface or you can run your first API request into your terminal.\nUpdated\n \nabout 2 months ago"},
{"url": "https://dev.writer.com/docs/prompt-engineering", "text": "Prompt engineering is the process of creating and fine-tuning the prompts used to generate text from a large language mode. This process is crucial in achieving the desired outcome and improving the performance of large language models. \nHere are some best practices for prompt engineering with large language models:\nUpdated\n \nabout 2 months ago"},
{"url": "https://dev.writer.com/docs/training-data-for-writer-llm", "text": "The Palmyra Large Language Model has demonstrated impressive performance in various natural language processing tasks. One of the key factors behind its success is the diverse and extensive training data it has been exposed to, which has given it expertise in business and professional writing.\nThe diverse range of datasets used to train the Palmyra Large Language Model ensures its proficiency in business and professional writing. With a thorough understanding of various domains, including web content, news articles, encyclopedic knowledge, technical documentation, and literature, Palmyra is well-equipped to tackle a multitude of natural language processing tasks with ease and accuracy.\nUpdated\n \n2 days ago"},
{"url": "https://dev.writer.com/docs/tokens", "text": "Language models understand \"tokens\" rather than characters or bytes. Each token represents a single unit of meaning, like a word or a group of words.\nFor example, consider the sentence \n\"I love to play soccer.\"\n The model would break this sentence down into tokens like \n\"I,\"\n \n\"love,\"**\n \"to,\"** \n\"play,\"\n and \n\"soccer.\"\n These tokens are then used by the model to make predictions about what might come next in the sentence, like \"with\" or \"on.\"\nThe idea is that by breaking the text into smaller pieces, the model can better understand the meaning and context of each word. This helps the model make more accurate predictions and generate more natural language.\nThink of it like cooking a meal. If you have all the ingredients chopped up into small pieces, it's easier to mix and cook them together to make a delicious dish. Tokens in large language models work in a similar way, making it easier for the model to process and understand the input text.\nTokenization is the process of breaking input text down into smaller units. Depending on the tokenization method used by the large language model, a single word can be represented by one token in simple text, while in others, a single word may be broken down into multiple sub-word tokens in complex texts. \nOur generation models support up to \n2048 tokens\n and our vocabulary of tokens is created using Byte Pair Encoding.\nUpdated\n \nabout 2 months ago"},
{"url": "https://dev.writer.com/docs/generation", "text": "Our models learn language by reading text from the internet. Given a sentence, such as I like to play with legos, the model is asked to repeatedly predict what the next token [?] is:\n**I [?]\nI like [?]\nI like to [?]\nI like to play [?]\nI like to play with [?]**\nThe model learns that the word \u201cto\u201d is quite likely to follow the word \u201clike\u201d in English, and that the word \u201cplay\u201d is likely to follow the word \u201cwith\u201d and so on. \nLikelihood refers to the probability that a given sequence of tokens (e.g., words or characters) will be generated by the model. The likelihood of a token can be thought of as a number (typically between -15 and 0) that quantifies a model's level of surprise that this token was used in a sentence. \nIf a token has a low likelihood, it means the model did not expect this token to be used. Conversely, if a token has a high likelihood, the model was confident that it would be used. Conversely, if a token has a high likelihood, the model was confident that it would be used.\nThe likelihood score can be used to evaluate the quality of the model's output, representing the model's confidence that a given output is the most likely sequence of tokens given the input prompt and the target data.\nThe same prompt may yield different outputs each time you hit \"generate\" and so, temperature is a parameter used to control the randomness and diversity of the generated output. \nThe temperature scale ranges from 0 to 1. A temperature of 1.0 means that the model is generating outputs with its normal level of confidence, based on the probabilities it has learned during training. \nA higher temperature will increase the randomness of the output, allowing for more diverse and unexpected results. A lower temperature will make the model more conservative in its predictions, generating outputs that are more likely to align with what it has learned during training.\nMost people will find that a temperature of 1 is a good starting point.\nWith longer prompts, the model becomes more confident in its predictions, so you can raise the temperature higher for a diverse and creative output without the output being too off topic. In contrast, using high temperatures on short prompts can lead to outputs being very unstable.\nWe recommend using lower temperature values for tasks like classification, entity extraction, or question answering, and use higher temperature values for tasks like content or idea generation.\nTop-k sampling and top-p sampling are methods for picking the output tokens by controlling the quality and diversity of the generated output.\nTop-p \nrefers to selecting the top p% of the most likely tokens (e.g., words or characters) based on the predicted probabilities of the model, and then randomly selecting one of the top p tokens to generate the next word.\nTop-p defaults to 1 but accepts any number between 0 and 1. Top-p is an alternative way of controlling the randomness of the generated text. When using top-p, make sure that temperature is set to 1. Generally, top-p will provide better control where your model is expected to generate text with accuracy and correctness.\nTop-k \nrefers to selecting the k tokens with the highest predicted probabilities and then randomly selecting one of these k tokens to generate the next word. This method is used to generate outputs that are more likely to be accurate, but may be less diverse. \nTop-k defaults to 0, but it will accept\n \nan integer between 1 and 50,400. \nTop-k refers to the number of tokens that will be sampled, sorted by probability with all tokens beneath the k'th token \u200cnot sampled. A lower value can improve quality by removing the long tail of less likely tokens and making it less likely to go off topic.\nUpdated\n \nabout 2 months ago"},
{"url": "https://dev.writer.com/docs/custom-models", "text": "Training large language models is only required when you need to teach the model something extremely niche, like your company's unique knowledge base or specific domain knowledge.  Common knowledge, like the colour of the sky, does not require training. \nThere are several reasons one might choose to train a custom language model:\nToken likelihood is a useful tool for model evaluation. For instance, let's say you've trained a custom model and would like to know how much it's improved over the default model - you could use token likelihoods to compare the performance of the models on some held-out text.\nUpdated\n \nabout 2 months ago"},
{"url": "https://dev.writer.com/docs/text-completion", "text": "The \ncompletions\n endpoint can be used for a wide variety of tasks  - you input some text as a prompt, and the model will generate a text completion that attempts to match the given context or pattern. \nFor example, if you give the API the prompt, \u201cAs Shakespeare said, \u201cAll the world's a stage, it'll return \u201cand all the men and women merely players.\u201d with high probability. \nThis means you can \"program\" the model by providing instructions or just a few examples of what you'd like it to do. Its success generally depends on the complexity of the task and the quality of your prompt. \nSee\n prompt engineering\n to guide you through this. \nWhile all prompts result in completions, text completion can be seen as its own task in instances where you want the model to pick up a paragraph or phrase where you left off.\nUpdated\n \n3 months ago"},
{"url": "https://dev.writer.com/docs/best-practices", "text": "Hyperparameters\nAs a starting point, aim for at least a few hundred examples.\nGenerally, the more data the better the model will adapt to a particular task. We recommend using the default values for all hyperparameters:\nChanging hyperparameters values from default may either increase or decrease the quality of the tuned model. Several rounds of experiments might be needed to find better models.\nSmaller models tend to take more epochs to train.\n\nModels can be badly trained for a number of reasons. For example, due to small dataset, large learning rates, or an insufficient number of epochs. Common observations include bad or incomplete responses marked with the \n token. In this case, you might have to retrain the model with a different set of hyperparameter. In the future, we will allow logging the training process to W&B for inspection.\nUpdated\n \n3 months ago"},
{"url": "https://dev.writer.com/docs/fine-tuning", "text": "How to train large language models for your use case.\nNow onto finetuning - an approach to transfer learning - it's a method for developing a model that's unique to your use case. Fine-tuning improves the capabilities of models by providing:\nLarge language models are pre-trained on a massive amount of text from the public Internet. When given a prompt with just a few examples, they can frequently comprehend the task you are attempting to accomplish and provide a useful response. This is called \"few-shot learning.\"\nFine-tuning improves on few-shot learning by training on many more examples than can fit in a prompt, letting you achieve better results on a wide number of tasks. Once a model has been  fine-tuned, you'll no longer be required to provide examples in the prompt. This reduces expenses and enables requests with reduced latency.\nAnother way of putting it is that fine-tuning a model is like customizing a product to better fit your specific needs. Imagine you bought a pre-made cake mix, and now you want to add some personal touches to make it special for a party. You might add extra ingredients, like nuts or fruit, or adjust the baking time to make the cake perfect for your occasion. This is similar to fine-tuning a pre-trained model.\nIn Writer\u2019s case, we finetune the model with your data to ensure that its output align with your style, your brand voice, and customized for your specific use cases - taking into account from word and character length to fact verification. \nHow fine-tuning lets you get the most out of the models: \n1. Improved Accuracy:\n Fine-tuning can lead to improved accuracy as compared to a model trained from scratch. This is because the pre-trained model has already been trained on a large variety of data and can provide a better starting point for further training.\n2. Reduced Training Time:\n Fine-tuning a model can require much less time than training a model from scratch, as the pre-trained model already contains many of the necessary parameters.\n3. Ease of Use:\n Fine-tuning a model is much easier than training a model from scratch, as all that is required is to adjust the existing parameters of the pre-trained model.\n4. Transfer Learning:\n Fine-tuning a model can provide the ability to transfer knowledge from one domain to another. This is because the pre-trained model has already been trained on data from one domain and can be used to quickly and effectively train a model for another domain.\n5. Increased Performance:\n Fine-tuning a model can lead to increased performance on unseen data as compared to training a model from scratch. This is because the pre-trained model has already been exposed to a variety of data and can provide a better starting point for further training.\nFine-tuning Palmyra involves training the model on a specific task or dataset after it has been pre-trained on a large corpus to learn general language features. When fine-tuning, you can choose to update the weights of some or all layers of the model. The difference between fine-tuning 1-2 layers or all layers is in how much of the model is being updated during this process.\n**1. Fine-tuning 1-2 layers: In this approach, only the last 1 or 2 layers of the model are trained or updated, while the weights of the other layers are kept fixed. This assumes that the lower layers have already learned useful language features during pre-training, and only the final layers need to be adapted to the specific task. This method is computationally less expensive and can lead to faster convergence. However, it may not be as effective in adapting the model to the new task, especially if the task is very different from the pre-training data.\n**2. Fine-tuning all layers: In this approach, the weights of all layers of the model are updated during fine-tuning. This can allow the model to adapt more effectively to the new task, as it has more flexibility in learning task-specific features. However, this method is more computationally expensive, requires more training time, and may be prone to overfitting if the fine-tuning dataset is small.\nIn summary, fine-tuning 1-2 layers is a more efficient approach but may not be as effective in adapting the model to the new task, while fine-tuning all layers may provide better performance at the cost of increased computational resources and training time. The choice between the two depends on the specific task, dataset size, and available resources.\nP-tuning is a more efficient way of adapting pre-trained language models for various tasks compared to fine-tuning. The Palmyra LLM customization service allows using one pre-trained model for many tasks without adjusting all the model's parameters. For instance, with the Palmyra-Large 20B model, only a small amount of parameters need to be trained and stored for each task. This is much less than the large amount of data required in fine-tuning, which can be around 40 GB per task. P-tuning also prevents problems like catastrophic forgetting, which can happen during fine-tuning.\nP-tuning is different from prompt engineering, which involves optimizing text prompts either manually or automatically. Instead, p-tuning uses virtual prompt embeddings that can be improved using gradient descent. These virtual tokens are 1D vectors with the same dimensions as real token embeddings. During training and testing, these continuous token embeddings are inserted before the real ones. As a result, the language model can respond differently to the same prompt when combined with different virtual tokens.\nUpdated\n \nabout 1 month ago"},
{"url": "https://dev.writer.com/docs/prepare-training-data", "text": "Here are some guidelines that we recommend as you prepare your dataset for finetuning: \nData formatting\nTo fine-tune a model, you'll need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\"). This is notably different from using our foundational models, where you might input detailed instructions or multiple examples in a single prompt.\nThe dataset must be a JSONL file with each line containing a prompt-completion pair illustrating an example of your task. We advise fine-tuning each model for a for a single, specific task.\nSee example of a \nJSONL\n file with this prompt-completion pair: \nGeneral best practices\nTo get the best results, use more examples. This is especially important for models that work better with prompts. You should provide at least 500 examples, preferably chosen by humans. \nThere's a linear increase in performance with every doubling of examples. This is usually the best and most reliable way of improving performance.\nUpdated\n \nabout 1 month ago"},
{"url": "https://dev.writer.com/docs/use-cases", "text": "Here are a few use cases that Writer covers with its generative AI capabilities\nFill-mask\nFill mask is a technique used in generative AI that allows users to input a partial sentence and have the AI system complete the sentence. Essentially, this is the task of masking some of the words in a sentence and predicting which words should replace those masks. This is possible because the AI system has been trained on a large amount of data and can generate new sentences that are similar to the ones it has seen before.\nText2text generation\nText2text generation is a method of creating text by using a neural network to generate new text from a given input. For example, a text2text model could be used to generate a summary of a news article based on the original article or a user could input a short story and the AI could generate a continuation of that story.\nSentence similarity\nSentence similarity is the process of determining how similar two sentences are in meaning. This is often done by comparing the words in each sentence and determining how often those words are used in similar contexts. For example, the sentences \"I like to eat apples\" and \"I enjoy eating apples\" are more similar than the sentences \"I like to eat apples\" and \"I swim in the ocean.\"\nQuestion answering\nWriter\u2019s AI is capable of answering questions based on information that's been fed to our model. It usually involves taking information from a given text and using it to generate a response that is relevant to the question. For example, you may ask Writer\u2019s AI to provide you with a plot summary of a book or ask why the sky is blue. \nText classification\n\u200b\u200bText classification is a process of assigning a class label to a piece of text. This is often done by first building a large language model, which is then used to generate labels for new pieces of text. For example, a language model might be used to label a piece of text as \"positive\" or \"negative\" based on the sentiment of the text.\nZero-shot classification\nZero-shot classification is a method of text classification that doesn't require any training data. Instead, it relies on large language models that have been trained on a large amount of data. These models can learn the general structure of language and can therefore classify new text without any training data.\nA relatable example of this would be if you were to take a large corpus of text, such as all of Wikipedia, and train a language model on it. This model would then be able to take any new text, such as a news article, and classify it according to its topic.\nSummarization\nSimply put, Writer\u2019s AI is capable of summarizing any piece of text. For example, a reader wants to get the gist of a long article without reading the entire thing and would request a summary of this article. \nText generation \nLastly, what we all know generative AI to be, text generation is the act of producing new text from large language models. Emails, summaries, essays and such can all be generated as an example.\nUpdated\n \nabout 2 months ago"},
{"url": "https://dev.writer.com/docs/hyperparameters", "text": "As a starting point, aim for at least a few hundred examples for fine-tuning  a model. Generally, the more data the better the model will adapt to a particular task. Additionally, tweaking the hyperparameters for fine-tuning can often lead to a model that produces higher quality completions. Hyperparameters are parameters that are used to control the learning process. \nWe recommend using the default values for all hyperparameters:\nBatch size\n is the number of training samples (e.g., sentences, paragraphs) used to update the model's parameters during each iteration of training. In general, we've found that larger batch sizes tend to work better for larger datasets.\nEpochs \nrefers to the model being trained one full cycle through the dataset. The number of epochs you should set depends on the number of training examples in your dataset and the size of the model you're starting from. Generally, larger models and datasets (many thousands of training examples) will need fewer epochs to train. We recommend slightly overtraining your models. \nLearning rate\n is used to set the fine-tuning learning rate by determining how quickly the model updates the concepts it has learned.\nChanging hyperparameters values from default may either increase or decrease the quality of the tuned model. Several rounds of experiments might be needed to find better models.\nSmaller models tend to take more epochs to train.\nModels can be badly trained for a number of reasons: a small dataset, large learning rates, or an insufficient number of epochs. In these cases, you might have to retrain the model with a different set of hyperparameters. \nIn the future, we will allow logging the training process to W&B for inspection.\nUpdated\n \n3 months ago"},
{"url": "https://dev.writer.com/docs/toxic-check", "text": "We developed an independent system that classifies input and output textual data and predicts its level of toxicity. Our model was trained on a distinct dataset that included both toxic and nontoxic examples. Each prompt is sent through the API to one of our large language models, and the generated text is analyzed and classified by a model that predicts the text's toxicity level. This probability lies between 0 and 1, where 0 indicates toxic classes and 1 indicates nontoxic classes.\nThe text completion API methods return a JSON response with two fields, prompt labels and completion labels, that indicate the toxicity scores for each part independently. The field class name contains a binary class label determined by score, where a value less than 0.5 indicates toxicity and a value greater than 0.5 indicates nontoxicity.\nUpdated\n \n4 months ago"},
{"url": "https://dev.writer.com/docs/models", "text": "The Writer LLM service enables you to customize and use the Writer LLMs outlined below.\nThese large language models have been pre-trained on a massive amount of Internet text. Pre-training involves taking a mathematical model with random mathematical parameters (weights) and adjusting those weights iteratively in response to discrepancies between the model's output and a comparison point indicating the expected output. The most common training method for large language models is next-word prediction over massive amounts of text.\nPalmyra Small is the fastest of Writer\u2019s LLMs and can perform important tasks such as text parsing, simple classification, address correction, and keyword recognition. Providing more context drives better performance.\nGood at: Text parsing, simple classification, address correction, and keyword recognition\n\n\nPalmyra Base is extremely powerful as well as incredibly fast. This model excels at many nuanced tasks such as sentiment classification and summarization. Palmyra Base is also effective as a general service chatbot, answering questions and performing Q&A.\nCompetent in: complex classification, text sentiment, and summarization\n\n\nCamel-5b is a trained large language model that follows instructions. Based on Palmyra-Base is trained on ~70k instruction & response fine tuning records generated by Writer Team from the InstructGPT paper, including brainstorming, classification, closed quality assurance, generation, information extraction, open quality assurance, and summarization.\nPalmyra Large is the most capable model family, capable of performing any task that the other models can, often with less instruction. Palmyra Large is good at comprehending the text's intent, solving logic problems, and explaining character motivations.\nGood at: Few-shots, cause and effect, and audience summarization\n\n\nInstructPalmyra is the most capable model. It can perform any tasks that the other models are able to, often with higher quality, longer output, and better instruction-following.\nGood at: Zero-shots, cause and effect\n\n\nPalmyra-R models are a general-purpose fine-tuning recipe for retrieval-augmented generation, combining pre-trained parametric and non-parametric memory for language generation. \nIt is more capable than\u00a0the GPT-3\u00a0and GPT-3.5 models, able to perform more complex tasks, and comes in three flavors: General, Healthcare, and Fintech. It is available on-premise or via\u00a0an API.\nSame capabilities as the Palmyra-E mode but with ~80K context length. still in early testing stage\nUpdated\n \n19 days ago"},
{"url": "https://dev.writer.com/docs/go-sdk", "text": "Writer authenticates your API requests using your account\u2019s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their team as a developer.\nUpdated\n \n19 days ago"},
{"url": "https://dev.writer.com/docs/python-sdk", "text": "Writer authenticates your API requests using your account\u2019s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their team as a developer.\nUpdated\n \n19 days ago"},
{"url": "https://dev.writer.com/docs/overview", "text": "Libraries and tools for interacting with your Writer integration.\nWriter\u2019s server-side helper libraries (also known as Server-side SDKs) reduce the amount of work required to use Writer\u2019s REST APIs, starting with reducing the boilerplate code you have to write. Below are the installation instructions for these libraries in a variety of popular server-side programming languages.\nYou can access certain Writer products and features in the beta stage with beta SDKs. The versions of these beta SDKs have the beta or b suffix, for example, 1.0b3 in Python and 1.0-beta.3 in other language SDKs. Try these beta SDKs and share feedback with us before the features reach the stable phase. To learn more about how to use the beta SDKs, read the readme file in the GitHub repository of the individual language SDKs.\nUpdated\n \nabout 2 months ago"},
{"url": "https://dev.writer.com/docs/usage-policies", "text": "Please review the following policies to ensure responsible usage of our API: \nContent Policy\nThe Writer API doesn't allow for the generation of the following types of content: \nPlatform Policy\nFailure to comply with this policy may result in suspension or termination of access to the API.\nWriter reserves the right to take necessary measures to enforce this policy and protect the system and other users from harm.\nThis policy may be revised at any time without prior notice. Users are responsible for regularly checking the policy for updates.\nUpdated\n \n2 months ago"},
{"url": "https://dev.writer.com/page/security", "text": "All security bugs in Writer are taken seriously and should be reported by emailing \n[email\u00a0protected]\n. This will be delivered to a subset of the core team who handle security issues.\nYour email will be acknowledged within 24 hours, and you\u2019ll receive a more detailed response to your email within 48 hours indicating the next steps in handling your report.\nAfter the initial reply to your report, the security team will endeavor to keep you informed of the progress being made towards a fix and full announcement, and may ask for additional information or guidance surrounding the reported issue. These updates will be sent at least every five days, in practice, this is more likely to be every 24-48 hours.\nSecurity bugs in third party systems should be reported to their respective maintainers and can also be coordinated through the Writer Security email.\nHere is the security disclosure policy for Writer\nThe security report is received and is assigned a primary handler. This person will coordinate the fix and release process. The problem is confirmed and a list of all affected versions is determined. Code is audited to find any potential similar problems. Fixes are prepared for all releases which are still under maintenance. These fixes are not committed to the public but rather held locally pending the announcement.\nEvery effort will be made to handle the bug in as timely a manner as possible, however, it\u2019s important that we follow the release process above to ensure that the disclosure is handled in a consistent manner.\nIf you have suggestions on how this process could be improved please submit a pull request or email \n[email\u00a0protected]\n to discuss."},
{"url": "https://dev.writer.com/docs/integrations", "text": "Writer for Chrome\nInstall Writer for Chrome to unlock on-brand content at scale. Improve your writing with line-by-line writing feedback, anywhere you use Chrome. Say goodbye to wordiness, over-formality, and all manner of writing errors.Stay consistent and on message with a custom, automated styleguide, use your latest terms, communicate in a healthy way, and be super efficient with autocorrect and autocomplete.\nWriter for Figma\nEverything you need to organize content design. Use the Writer + Figma plugin to make sure all your content is clear, consistent, and error-free. \nWriter for Microsoft Word\n \nUse Writer to check your writing in Microsoft Word. This add-in takes your customized guidelines from Writer and applies it across your documents, in both desktop Word and Word 365.\nWriter for Contentful \nWriter is generative AI built for your business. The collaboration between Writer and Contentful enables marketers to make sure all of the content published through Contentful is clear, consistent, on-brand, and error-free.\nFor help with installation, check out our help center \nhere\n.\nUpdated\n \n3 months ago"},
{"url": "https://dev.writer.com/docs/integration-security-guide", "text": "Ensure compliance and secure communications between Writer and your server.\nTLS refers to the process of securely transmitting data between the client\u2014the app or browser that your customer is using\u2014and your server. This was originally performed using the SSL (Secure Sockets Layer) protocol. However, this is outdated and no longer secure, and has been replaced by TLS. The term \u201cSSL\u201d continues to be used colloquially when referring to TLS and its function to protect transmitted data.\nPayment pages must make use of a modern version of TLS (e.g., TLS 1.2) as it significantly reduces the risk of you or your customers being exposed to a man-in-the-middle attack. TLS attempts to accomplish the following:\nAdditionally, your customers are more comfortable sharing sensitive information on pages visibly served over HTTPS, which can help increase your customer conversion rate.\nIf need be, you can test your integration without using HTTPS, and enable it once you\u2019re ready to accept live charges. However, all interactions between your server and Writer must use TLS 1.2 (i.e, when using our libraries).\nSERVING RESOURCES SECURELY\n\nYou should make sure that any resources (JavaScript, CSS, images, etc.) are also served over TLS to avoid a mixed content warning being shown to your customers in their browser.\nA digital certificate\u2014a file issued by a certification authority (CA)\u2014is needed in order to use TLS. When installed, this certificate assures the client that it\u2019s really communicating with the server it expects to be talking to, not an impostor. You should get a digital certificate from a reputable certificate provider, such as:\nConceptually, setting up TLS is very straightforward: a certificate is purchased from a suitable provider, and then your server is configured to use it. The actual process does tend to be somewhat complex, and we recommend you follow the installation guide of the provider you use.\nAs TLS is a complex suite of cryptographic tools, it\u2019s easy to miss a few details. We recommend using the SSL Server Test by Qualys SSL Labs to make sure you have everything set up in a secure way.\nIt can be a security risk to include JavaScript from other sites as your security becomes dependent on theirs. If they\u2019re ever compromised, an attacker may be able to execute arbitrary code on your page. In practice, many sites make use of JavaScript for services like Google Analytics, even on secure pages. Nonetheless, it\u2019s something to be aware of, and ideally minimize.\nIf you\u2019re making use of webhooks, we recommend using TLS for the endpoint to avoid traffic being intercepted and the notifications altered (sensitive information is never included in a webhook event).\nWhile complying with the Data Security Standards is important, it shouldn\u2019t be where you stop thinking about security. Some good resources to learn about web security are:\nUpdated\n \n3 months ago"},
{"url": "https://dev.writer.com/docs/rate-limits", "text": "What are rate limits?\nA rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time. \nNote: A request is defined as a single API call made by an API user. Requests can be made to any endpoint within the API.\nWhy do we have rate limits?\nWhat happens if I encounter a rate limit error? \nIf you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed.\nEach API key is allowed a certain number of requests per hour. If you exceed this limit, you will receive a 429 error response.\nWhat are the rate limits for the Writer API? \nHow can I check my rate limit status? \nYou can check your current rate limit status at any moment using the developer dashboard.\nTo check your current rate limit status, you can include the following headers in your request:\nWhen should I consider requesting a rate limit increase? \nOur default rate limits help us maintain stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. \nThis rate limiting policy may change from time to time. API users will be notified of any changes through the API documentation or by email.\nWriter API calls of various categories are subject to differing rate constraints. Furthermore, the Enterprise API has its own set of restrictions.\nUpdated\n \n3 months ago"},
{"url": "https://dev.writer.com/reference/contentcheck", "text": ""},
{"url": "https://dev.writer.com/docs/errors", "text": "Writer uses conventional HTTP response codes to indicate the success or failure of an API request. In general:\nHere are some common error codes that can occur:\nThese are just a few of the many error codes that can occur when making API requests. Understanding the meaning of these codes can help you diagnose and resolve issues when working with APIs.\nUpdated\n \nabout 1 month ago"},
{"url": "https://dev.writer.com/docs/versioning", "text": "The Writer API is versioned. The API version name is based on the date when the API version was released. For example, the current version is \n2023-21-01\n. \nWhen backwards-incompatible changes are made to the API, a new, dated version is released. The current version is \n2023-21-01.\n Read our API upgrades guide to see our API changelog and to learn more about backwards compatibility.\nThe changelog lists every available version. To set the API version on a specific request, send a Writer-Version header.\nUpdated\n \nabout 1 month ago"},
{"url": "https://dev.writer.com/docs/authentication", "text": "The Writer API uses API keys for authentication - please reach out to your admin to generate an API key for your requests. \nDo not share your API keys\n with others or expose it in any client-side code (browsers, apps). You should never, ever store these values in any publicly accessible location.  We recommend storing them as environment variables or accessed from a key management service. \nAuthorization Type \nWriter uses bearer authentication (also called token authentication), which is an HTTP authentication scheme that involves security tokens called bearer tokens. Bearer authentication (also called token authentication) is an \nHTTP authentication scheme\n that involves security tokens called bearer tokens. The name \u201cBearer authentication\u201d can be understood as \u201cgive access to the bearer of this token.\u201d The bearer token is a cryptic string, usually generated by the server in response to a login request. The client must send this token in the \nAuthorization\n header when making requests to protected resources:\nThe Bearer authentication scheme was originally created as part of \nOAuth 2.0\n in \nRFC 6750\n, but is sometimes also used on its own. Similarly to \nBasic authentication\n, Bearer authentication should only be used over HTTPS (SSL).\nUpdated\n \nabout 1 month ago"},
{"url": "https://dev.writer.com/docs/introduction", "text": "The Writer API is organized around REST. Our API has predictable resource-oriented URLs, accepts form-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes, authentication, and verbs.\nThe Writer API differs for every account as we release new versions and tailor functionality. Log in to see docs customized to your version of the API with your test key and data.\nUpdated\n \nabout 1 month ago"}
]