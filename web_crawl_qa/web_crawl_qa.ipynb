{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawl Q&A\n",
    "\n",
    "This app will allow you to ask questions based on a website's content and get a clear, summarized answer as long as the information exists. It is quite similar in structure to the File Q&A example, but scrapes webpages for data rather than files. For this example, we'll be scraping the [Writer docs](https://dev.writer.com/docs) so we can ask questions about using Writer models and APIs.\n",
    "\n",
    "At a high level, it will:\n",
    "* Use the Scrapy framework to gather text content from a specific domain\n",
    "* Split the content into relatively small pieces\n",
    "* Use a sentence similarity-capable language model to separate the most relevant pieces to the query\n",
    "* Use a generative language model to sumarize the useful information in those pieces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "Make sure you have a virtual environment selected if you don't want to install dependencies globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Using cached Scrapy-2.8.0-py2.py3-none-any.whl (272 kB)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.3.3-cp310-cp310-macosx_11_0_arm64.whl (706 kB)\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting writerai\n",
      "  Using cached writerai-0.4.0-py3-none-any.whl (85 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting service-identity>=18.1.0\n",
      "  Using cached service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zope.interface>=5.1.0\n",
      "  Using cached zope.interface-6.0-cp310-cp310-macosx_11_0_arm64.whl (202 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Using cached w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Using cached itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Using cached PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting tldextract\n",
      "  Using cached tldextract-3.4.1-py3-none-any.whl (92 kB)\n",
      "Collecting lxml>=4.3.0\n",
      "  Using cached lxml-4.9.2.tar.gz (3.7 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cssselect>=0.9.1\n",
      "  Using cached cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting Twisted>=18.9.0\n",
      "  Using cached Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Using cached itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Using cached queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: packaging in /Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages (from scrapy) (23.1)\n",
      "Requirement already satisfied: setuptools in /Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages (from scrapy) (65.5.0)\n",
      "Collecting pyOpenSSL>=21.0.0\n",
      "  Using cached pyOpenSSL-23.1.1-py3-none-any.whl (57 kB)\n",
      "Collecting cryptography>=3.4.6\n",
      "  Using cached cryptography-40.0.2-cp36-abi3-macosx_10_12_universal2.whl (5.1 MB)\n",
      "Collecting protego>=0.1.15\n",
      "  Using cached Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Using cached parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting requests>=2.26.0\n",
      "  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Using cached regex-2023.5.5-cp310-cp310-macosx_11_0_arm64.whl (288 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-2.0.0-cp310-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.15.1-cp310-cp310-macosx_11_0_arm64.whl (1.4 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.3-cp310-cp310-macosx_11_0_arm64.whl (13.9 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.2-cp310-cp310-macosx_12_0_arm64.whl (8.5 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.10.1-cp310-cp310-macosx_12_0_arm64.whl (28.8 MB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Collecting certifi==2022.12.07\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting marshmallow-enum==1.5.1\n",
      "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages (from writerai) (2.8.2)\n",
      "Collecting packaging\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: six==1.16.0 in /Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages (from writerai) (1.16.0)\n",
      "Collecting urllib3==1.26.12\n",
      "  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "Collecting charset-normalizer==2.1.1\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting dataclasses-json-speakeasy==0.5.8\n",
      "  Using cached dataclasses_json_speakeasy-0.5.8-py3-none-any.whl (26 kB)\n",
      "Collecting mypy-extensions==0.4.3\n",
      "  Using cached mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting pyparsing==3.0.9\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting typing-inspect==0.8.0\n",
      "  Using cached typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting idna==3.3\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting marshmallow==3.17.1\n",
      "  Using cached marshmallow-3.17.1-py3-none-any.whl (48 kB)\n",
      "Collecting requests>=2.26.0\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting typing-extensions==4.3.0\n",
      "  Using cached typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting pylint==2.16.2\n",
      "  Using cached pylint-2.16.2-py3-none-any.whl (530 kB)\n",
      "Collecting tomlkit>=0.10.1\n",
      "  Using cached tomlkit-0.11.8-py3-none-any.whl (35 kB)\n",
      "Collecting mccabe<0.8,>=0.6\n",
      "  Using cached mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting dill>=0.2\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting astroid<=2.16.0-dev0,>=2.14.2\n",
      "  Using cached astroid-2.15.4-py3-none-any.whl (278 kB)\n",
      "Collecting isort<6,>=4.2.5\n",
      "  Using cached isort-5.12.0-py3-none-any.whl (91 kB)\n",
      "Collecting tomli>=1.1.0\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages (from pylint==2.16.2->writerai) (3.5.0)\n",
      "Collecting cffi>=1.12\n",
      "  Using cached cffi-1.15.1-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp310-cp310-macosx_11_0_arm64.whl (173 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting attrs>=19.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting pyasn1-modules\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting pyasn1\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl (3.9 MB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Using cached Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Using cached incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Using cached constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting requests-file>=1.4\n",
      "  Using cached requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Using cached Pillow-9.5.0-cp310-cp310-macosx_11_0_arm64.whl (3.1 MB)\n",
      "Collecting lazy-object-proxy>=1.4.0\n",
      "  Using cached lazy_object_proxy-1.9.0-cp310-cp310-macosx_11_0_arm64.whl\n",
      "Collecting wrapt<2,>=1.11\n",
      "  Using cached wrapt-1.15.0-cp310-cp310-macosx_11_0_arm64.whl (36 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.2-cp310-cp310-macosx_10_9_universal2.whl (17 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tokenizers, sentencepiece, PyDispatcher, mypy-extensions, mpmath, incremental, constantly, zope.interface, wrapt, w3lib, urllib3, typing-extensions, tqdm, tomlkit, tomli, threadpoolctl, sympy, regex, queuelib, pyyaml, python-dotenv, pyparsing, pycparser, pyasn1, protego, pillow, numpy, networkx, mccabe, MarkupSafe, lxml, lazy-object-proxy, joblib, jmespath, itemadapter, isort, idna, fsspec, filelock, dill, cssselect, click, charset-normalizer, certifi, attrs, typing-inspect, scipy, requests, pyasn1-modules, packaging, nltk, jinja2, hyperlink, cffi, Automat, astroid, Twisted, torch, tiktoken, scikit-learn, requests-file, pylint, parsel, marshmallow, huggingface-hub, cryptography, transformers, torchvision, tldextract, service-identity, pyOpenSSL, marshmallow-enum, itemloaders, sentence_transformers, scrapy, dataclasses-json-speakeasy, writerai\n",
      "\u001b[33m  DEPRECATION: lxml is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for lxml ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "\u001b[33m  DEPRECATION: sentence_transformers is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for sentence_transformers ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed Automat-22.10.0 MarkupSafe-2.1.2 PyDispatcher-2.0.7 Twisted-22.10.0 astroid-2.15.4 attrs-23.1.0 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-2.1.1 click-8.1.3 constantly-15.1.0 cryptography-40.0.2 cssselect-1.2.0 dataclasses-json-speakeasy-0.5.8 dill-0.3.6 filelock-3.12.0 fsspec-2023.4.0 huggingface-hub-0.14.1 hyperlink-21.0.0 idna-3.3 incremental-22.10.0 isort-5.12.0 itemadapter-0.8.0 itemloaders-1.1.0 jinja2-3.1.2 jmespath-1.0.1 joblib-1.2.0 lazy-object-proxy-1.9.0 lxml-4.9.2 marshmallow-3.17.1 marshmallow-enum-1.5.1 mccabe-0.7.0 mpmath-1.3.0 mypy-extensions-0.4.3 networkx-3.1 nltk-3.8.1 numpy-1.24.3 packaging-21.3 parsel-1.8.1 pillow-9.5.0 protego-0.2.1 pyOpenSSL-23.1.1 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycparser-2.21 pylint-2.16.2 pyparsing-3.0.9 python-dotenv-1.0.0 pyyaml-6.0 queuelib-1.6.2 regex-2023.5.5 requests-2.28.1 requests-file-1.5.1 scikit-learn-1.2.2 scipy-1.10.1 scrapy-2.8.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 service-identity-21.1.0 sympy-1.11.1 threadpoolctl-3.1.0 tiktoken-0.3.3 tldextract-3.4.1 tokenizers-0.13.3 tomli-2.0.1 tomlkit-0.11.8 torch-2.0.0 torchvision-0.15.1 tqdm-4.65.0 transformers-4.28.1 typing-extensions-4.3.0 typing-inspect-0.8.0 urllib3-1.26.12 w3lib-2.1.1 wrapt-1.15.0 writerai-0.4.0 zope.interface-6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scrapy tiktoken sentence_transformers writerai python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping a website with Scrapy\n",
    "\n",
    "The Scrapy framework is a bit complicated at first, but it provides a lot of useful features for robust and customizable web scraping. It needs a specific project organization, and we can easily set that up with `scrapy startproject [name]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'crawler', using template directory '/Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages/scrapy/templates/project', created in:\n",
      "    /Users/heathexer/writer/writer-cookbook/web_crawl_qa/crawler\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd crawler\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject crawler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a few files and directories, but for this example all we need to do is create a basic spider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./crawler/crawler/spiders/writer_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"./crawler/crawler/spiders/writer_spider.py\"\n",
    "\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "class WriterSpider(CrawlSpider):\n",
    "    name = \"dev.writer.com\"\n",
    "    allowed_domains = [\"dev.writer.com\"]\n",
    "    start_urls = [\"https://dev.writer.com/docs/\"]\n",
    "\n",
    "    rules = [\n",
    "        Rule(LinkExtractor(), callback='parse', follow=False)\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Select all text that is contained in a <p> block\n",
    "        text = \"\\n\".join(response.selector.xpath(\"//p//text()\").extract()).strip()\n",
    "        return {\n",
    "            'url': response.url,\n",
    "            'text': text\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is highly dependent on your target site. For example, all of the pages we want to search happen to be accessible in one 'click' from the `start_urls`, so we can set `follow=False`. If this was not the case, we would need to set `follow=True` and probably add some additional rules to filter out pages we don't want. For a starter guide on Scrapy you can look [here](https://www.scrapingbee.com/blog/web-scraping-with-scrapy/), or for more detailed information and examples look at the [Scrapy docs](https://docs.scrapy.org/en/latest/index.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created out spider, we can run it with the aptly named `scrapy runspider [name]`. The -O flag tells it what file to put the gathered data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 15:30:31 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2023-05-03 15:30:31 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.10 (main, Apr  5 2023, 14:58:08) [Clang 11.1.0 ], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform macOS-13.1-arm64-arm-64bit\n",
      "2023-05-03 15:30:31 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'SPIDER_LOADER_WARN_ONLY': True}\n",
      "2023-05-03 15:30:31 [py.warnings] WARNING: /Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-05-03 15:30:31 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-05-03 15:30:31 [scrapy.extensions.telnet] INFO: Telnet Password: 63286887eb7e9401\n",
      "2023-05-03 15:30:31 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-05-03 15:30:31 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-05-03 15:30:31 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-05-03 15:30:31 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-05-03 15:30:31 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-05-03 15:30:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-05-03 15:30:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-05-03 15:30:31 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://dev.writer.com/docs> from <GET https://dev.writer.com/docs/>\n",
      "2023-05-03 15:30:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs> (referer: None)\n",
      "2023-05-03 15:30:32 [py.warnings] WARNING: /Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.\n",
      "  super().__init__(text=text, type=st, root=root, **kwargs)\n",
      "\n",
      "2023-05-03 15:30:32 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://dev.writer.com/docs> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n",
      "2023-05-03 15:30:33 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dev.writer.com/docs> from <GET https://dev.writer.com>\n",
      "2023-05-03 15:30:33 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dev.writer.com/reference/content> from <GET https://dev.writer.com/reference>\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/quickstart> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/recipes> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [py.warnings] WARNING: /Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.\n",
      "  super().__init__(text=text, type=st, root=root, **kwargs)\n",
      "\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/quickstart>\n",
      "{'url': 'https://dev.writer.com/docs/quickstart', 'text': 'Integrating Writer into your app or website can begin as soon as you create a Writer developer account, requiring only three steps:\\nWriter authenticates your API requests using your account’s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their organization as an org admin.\\nYou can run a test directly from the API interface or you can run your first API request into your terminal.\\nUpdated\\n \\nabout 2 months ago'}\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/recipes>\n",
      "{'url': 'https://dev.writer.com/recipes', 'text': ''}\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/edit/quickstart> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/edit/quickstart>\n",
      "{'url': 'https://dev.writer.com/edit/quickstart', 'text': 'Integrating Writer into your app or website can begin as soon as you create a Writer developer account, requiring only three steps:\\nObtain your API keys so Writer can authenticate your integration’s API requests\\nMake an API request to confirm everything is up and running \\nWriter authenticates your API requests using your account’s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\\n\\ufeff\\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their organization as an org admin.\\nYou can run a test directly from the API interface or you can run your first API request into your terminal.\\nAuthorization\\nVariable\\nhttpAuth\\nVariable\\n-1\\n+1\\n100\\n1234\\n8ball\\na\\nab\\nabc\\nabcd\\naccept\\naerial-tramway\\nairplane\\nalarm-clock\\nalien\\nambulance\\nanchor\\nangel\\nanger\\nangry\\nanguished\\nant\\napple\\naquarius\\naries\\narrow-backward'}\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs#content> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/prompt-engineering> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs>\n",
      "{'url': 'https://dev.writer.com/docs', 'text': 'Integrating Writer into your app or website can begin as soon as you create a Writer developer account, requiring only three steps:\\nWriter authenticates your API requests using your account’s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their organization as an org admin.\\nYou can run a test directly from the API interface or you can run your first API request into your terminal.\\nUpdated\\n \\nabout 2 months ago'}\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/training-data-for-writer-llm> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/tokens> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/generation> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/custom-models> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/prompt-engineering>\n",
      "{'url': 'https://dev.writer.com/docs/prompt-engineering', 'text': 'Prompt engineering is the process of creating and fine-tuning the prompts used to generate text from a large language mode. This process is crucial in achieving the desired outcome and improving the performance of large language models. \\nHere are some best practices for prompt engineering with large language models:\\nUpdated\\n \\nabout 2 months ago'}\n",
      "2023-05-03 15:30:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/text-completion> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/training-data-for-writer-llm>\n",
      "{'url': 'https://dev.writer.com/docs/training-data-for-writer-llm', 'text': 'The Palmyra Large Language Model has demonstrated impressive performance in various natural language processing tasks. One of the key factors behind its success is the diverse and extensive training data it has been exposed to, which has given it expertise in business and professional writing.\\nThe diverse range of datasets used to train the Palmyra Large Language Model ensures its proficiency in business and professional writing. With a thorough understanding of various domains, including web content, news articles, encyclopedic knowledge, technical documentation, and literature, Palmyra is well-equipped to tackle a multitude of natural language processing tasks with ease and accuracy.\\nUpdated\\n \\n2 days ago'}\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/tokens>\n",
      "{'url': 'https://dev.writer.com/docs/tokens', 'text': 'Language models understand \"tokens\" rather than characters or bytes. Each token represents a single unit of meaning, like a word or a group of words.\\nFor example, consider the sentence \\n\"I love to play soccer.\"\\n The model would break this sentence down into tokens like \\n\"I,\"\\n \\n\"love,\"**\\n \"to,\"** \\n\"play,\"\\n and \\n\"soccer.\"\\n These tokens are then used by the model to make predictions about what might come next in the sentence, like \"with\" or \"on.\"\\nThe idea is that by breaking the text into smaller pieces, the model can better understand the meaning and context of each word. This helps the model make more accurate predictions and generate more natural language.\\nThink of it like cooking a meal. If you have all the ingredients chopped up into small pieces, it\\'s easier to mix and cook them together to make a delicious dish. Tokens in large language models work in a similar way, making it easier for the model to process and understand the input text.\\nTokenization is the process of breaking input text down into smaller units. Depending on the tokenization method used by the large language model, a single word can be represented by one token in simple text, while in others, a single word may be broken down into multiple sub-word tokens in complex texts. \\nOur generation models support up to \\n2048 tokens\\n and our vocabulary of tokens is created using Byte Pair Encoding.\\nUpdated\\n \\nabout 2 months ago'}\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/generation>\n",
      "{'url': 'https://dev.writer.com/docs/generation', 'text': 'Our models learn language by reading text from the internet. Given a sentence, such as I like to play with legos, the model is asked to repeatedly predict what the next token [?] is:\\n**I [?]\\nI like [?]\\nI like to [?]\\nI like to play [?]\\nI like to play with [?]**\\nThe model learns that the word “to” is quite likely to follow the word “like” in English, and that the word “play” is likely to follow the word “with” and so on. \\nLikelihood refers to the probability that a given sequence of tokens (e.g., words or characters) will be generated by the model. The likelihood of a token can be thought of as a number (typically between -15 and 0) that quantifies a model\\'s level of surprise that this token was used in a sentence. \\nIf a token has a low likelihood, it means the model did not expect this token to be used. Conversely, if a token has a high likelihood, the model was confident that it would be used. Conversely, if a token has a high likelihood, the model was confident that it would be used.\\nThe likelihood score can be used to evaluate the quality of the model\\'s output, representing the model\\'s confidence that a given output is the most likely sequence of tokens given the input prompt and the target data.\\nThe same prompt may yield different outputs each time you hit \"generate\" and so, temperature is a parameter used to control the randomness and diversity of the generated output. \\nThe temperature scale ranges from 0 to 1. A temperature of 1.0 means that the model is generating outputs with its normal level of confidence, based on the probabilities it has learned during training. \\nA higher temperature will increase the randomness of the output, allowing for more diverse and unexpected results. A lower temperature will make the model more conservative in its predictions, generating outputs that are more likely to align with what it has learned during training.\\nMost people will find that a temperature of 1 is a good starting point.\\nWith longer prompts, the model becomes more confident in its predictions, so you can raise the temperature higher for a diverse and creative output without the output being too off topic. In contrast, using high temperatures on short prompts can lead to outputs being very unstable.\\nWe recommend using lower temperature values for tasks like classification, entity extraction, or question answering, and use higher temperature values for tasks like content or idea generation.\\nTop-k sampling and top-p sampling are methods for picking the output tokens by controlling the quality and diversity of the generated output.\\nTop-p \\nrefers to selecting the top p% of the most likely tokens (e.g., words or characters) based on the predicted probabilities of the model, and then randomly selecting one of the top p tokens to generate the next word.\\nTop-p defaults to 1 but accepts any number between 0 and 1. Top-p is an alternative way of controlling the randomness of the generated text. When using top-p, make sure that temperature is set to 1. Generally, top-p will provide better control where your model is expected to generate text with accuracy and correctness.\\nTop-k \\nrefers to selecting the k tokens with the highest predicted probabilities and then randomly selecting one of these k tokens to generate the next word. This method is used to generate outputs that are more likely to be accurate, but may be less diverse. \\nTop-k defaults to 0, but it will accept\\n \\nan integer between 1 and 50,400. \\nTop-k refers to the number of tokens that will be sampled, sorted by probability with all tokens beneath the k\\'th token \\u200cnot sampled. A lower value can improve quality by removing the long tail of less likely tokens and making it less likely to go off topic.\\nUpdated\\n \\nabout 2 months ago'}\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/custom-models>\n",
      "{'url': 'https://dev.writer.com/docs/custom-models', 'text': \"Training large language models is only required when you need to teach the model something extremely niche, like your company's unique knowledge base or specific domain knowledge.  Common knowledge, like the colour of the sky, does not require training. \\nThere are several reasons one might choose to train a custom language model:\\nToken likelihood is a useful tool for model evaluation. For instance, let's say you've trained a custom model and would like to know how much it's improved over the default model - you could use token likelihoods to compare the performance of the models on some held-out text.\\nUpdated\\n \\nabout 2 months ago\"}\n",
      "2023-05-03 15:30:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/text-completion>\n",
      "{'url': 'https://dev.writer.com/docs/text-completion', 'text': 'The \\ncompletions\\n endpoint can be used for a wide variety of tasks  - you input some text as a prompt, and the model will generate a text completion that attempts to match the given context or pattern. \\nFor example, if you give the API the prompt, “As Shakespeare said, “All the world\\'s a stage, it\\'ll return “and all the men and women merely players.” with high probability. \\nThis means you can \"program\" the model by providing instructions or just a few examples of what you\\'d like it to do. Its success generally depends on the complexity of the task and the quality of your prompt. \\nSee\\n prompt engineering\\n to guide you through this. \\nWhile all prompts result in completions, text completion can be seen as its own task in instances where you want the model to pick up a paragraph or phrase where you left off.\\nUpdated\\n \\n3 months ago'}\n",
      "2023-05-03 15:30:34 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://dev.writer.com/reference/contentcheck> from <GET https://dev.writer.com/reference/content>\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/best-practices> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/fine-tuning> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [py.warnings] WARNING: /Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages/scrapy/selector/unified.py:83: UserWarning: Selector got both text and root, root is being ignored.\n",
      "  super().__init__(text=text, type=st, root=root, **kwargs)\n",
      "\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/best-practices>\n",
      "{'url': 'https://dev.writer.com/docs/best-practices', 'text': 'Hyperparameters\\nAs a starting point, aim for at least a few hundred examples.\\nGenerally, the more data the better the model will adapt to a particular task. We recommend using the default values for all hyperparameters:\\nChanging hyperparameters values from default may either increase or decrease the quality of the tuned model. Several rounds of experiments might be needed to find better models.\\nSmaller models tend to take more epochs to train.\\n\\nModels can be badly trained for a number of reasons. For example, due to small dataset, large learning rates, or an insufficient number of epochs. Common observations include bad or incomplete responses marked with the \\n token. In this case, you might have to retrain the model with a different set of hyperparameter. In the future, we will allow logging the training process to W&B for inspection.\\nUpdated\\n \\n3 months ago'}\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/fine-tuning>\n",
      "{'url': 'https://dev.writer.com/docs/fine-tuning', 'text': 'How to train large language models for your use case.\\nNow onto finetuning - an approach to transfer learning - it\\'s a method for developing a model that\\'s unique to your use case. Fine-tuning improves the capabilities of models by providing:\\nLarge language models are pre-trained on a massive amount of text from the public Internet. When given a prompt with just a few examples, they can frequently comprehend the task you are attempting to accomplish and provide a useful response. This is called \"few-shot learning.\"\\nFine-tuning improves on few-shot learning by training on many more examples than can fit in a prompt, letting you achieve better results on a wide number of tasks. Once a model has been  fine-tuned, you\\'ll no longer be required to provide examples in the prompt. This reduces expenses and enables requests with reduced latency.\\nAnother way of putting it is that fine-tuning a model is like customizing a product to better fit your specific needs. Imagine you bought a pre-made cake mix, and now you want to add some personal touches to make it special for a party. You might add extra ingredients, like nuts or fruit, or adjust the baking time to make the cake perfect for your occasion. This is similar to fine-tuning a pre-trained model.\\nIn Writer’s case, we finetune the model with your data to ensure that its output align with your style, your brand voice, and customized for your specific use cases - taking into account from word and character length to fact verification. \\nHow fine-tuning lets you get the most out of the models: \\n1. Improved Accuracy:\\n Fine-tuning can lead to improved accuracy as compared to a model trained from scratch. This is because the pre-trained model has already been trained on a large variety of data and can provide a better starting point for further training.\\n2. Reduced Training Time:\\n Fine-tuning a model can require much less time than training a model from scratch, as the pre-trained model already contains many of the necessary parameters.\\n3. Ease of Use:\\n Fine-tuning a model is much easier than training a model from scratch, as all that is required is to adjust the existing parameters of the pre-trained model.\\n4. Transfer Learning:\\n Fine-tuning a model can provide the ability to transfer knowledge from one domain to another. This is because the pre-trained model has already been trained on data from one domain and can be used to quickly and effectively train a model for another domain.\\n5. Increased Performance:\\n Fine-tuning a model can lead to increased performance on unseen data as compared to training a model from scratch. This is because the pre-trained model has already been exposed to a variety of data and can provide a better starting point for further training.\\nFine-tuning Palmyra involves training the model on a specific task or dataset after it has been pre-trained on a large corpus to learn general language features. When fine-tuning, you can choose to update the weights of some or all layers of the model. The difference between fine-tuning 1-2 layers or all layers is in how much of the model is being updated during this process.\\n**1. Fine-tuning 1-2 layers: In this approach, only the last 1 or 2 layers of the model are trained or updated, while the weights of the other layers are kept fixed. This assumes that the lower layers have already learned useful language features during pre-training, and only the final layers need to be adapted to the specific task. This method is computationally less expensive and can lead to faster convergence. However, it may not be as effective in adapting the model to the new task, especially if the task is very different from the pre-training data.\\n**2. Fine-tuning all layers: In this approach, the weights of all layers of the model are updated during fine-tuning. This can allow the model to adapt more effectively to the new task, as it has more flexibility in learning task-specific features. However, this method is more computationally expensive, requires more training time, and may be prone to overfitting if the fine-tuning dataset is small.\\nIn summary, fine-tuning 1-2 layers is a more efficient approach but may not be as effective in adapting the model to the new task, while fine-tuning all layers may provide better performance at the cost of increased computational resources and training time. The choice between the two depends on the specific task, dataset size, and available resources.\\nP-tuning is a more efficient way of adapting pre-trained language models for various tasks compared to fine-tuning. The Palmyra LLM customization service allows using one pre-trained model for many tasks without adjusting all the model\\'s parameters. For instance, with the Palmyra-Large 20B model, only a small amount of parameters need to be trained and stored for each task. This is much less than the large amount of data required in fine-tuning, which can be around 40 GB per task. P-tuning also prevents problems like catastrophic forgetting, which can happen during fine-tuning.\\nP-tuning is different from prompt engineering, which involves optimizing text prompts either manually or automatically. Instead, p-tuning uses virtual prompt embeddings that can be improved using gradient descent. These virtual tokens are 1D vectors with the same dimensions as real token embeddings. During training and testing, these continuous token embeddings are inserted before the real ones. As a result, the language model can respond differently to the same prompt when combined with different virtual tokens.\\nUpdated\\n \\nabout 1 month ago'}\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/prepare-training-data> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/use-cases> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/prepare-training-data>\n",
      "{'url': 'https://dev.writer.com/docs/prepare-training-data', 'text': 'Here are some guidelines that we recommend as you prepare your dataset for finetuning: \\nData formatting\\nTo fine-tune a model, you\\'ll need a set of training examples that each consist of a single input (\"prompt\") and its associated output (\"completion\"). This is notably different from using our foundational models, where you might input detailed instructions or multiple examples in a single prompt.\\nThe dataset must be a JSONL file with each line containing a prompt-completion pair illustrating an example of your task. We advise fine-tuning each model for a for a single, specific task.\\nSee example of a \\nJSONL\\n file with this prompt-completion pair: \\nGeneral best practices\\nTo get the best results, use more examples. This is especially important for models that work better with prompts. You should provide at least 500 examples, preferably chosen by humans. \\nThere\\'s a linear increase in performance with every doubling of examples. This is usually the best and most reliable way of improving performance.\\nUpdated\\n \\nabout 1 month ago'}\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/hyperparameters> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/toxic-check> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/use-cases>\n",
      "{'url': 'https://dev.writer.com/docs/use-cases', 'text': 'Here are a few use cases that Writer covers with its generative AI capabilities\\nFill-mask\\nFill mask is a technique used in generative AI that allows users to input a partial sentence and have the AI system complete the sentence. Essentially, this is the task of masking some of the words in a sentence and predicting which words should replace those masks. This is possible because the AI system has been trained on a large amount of data and can generate new sentences that are similar to the ones it has seen before.\\nText2text generation\\nText2text generation is a method of creating text by using a neural network to generate new text from a given input. For example, a text2text model could be used to generate a summary of a news article based on the original article or a user could input a short story and the AI could generate a continuation of that story.\\nSentence similarity\\nSentence similarity is the process of determining how similar two sentences are in meaning. This is often done by comparing the words in each sentence and determining how often those words are used in similar contexts. For example, the sentences \"I like to eat apples\" and \"I enjoy eating apples\" are more similar than the sentences \"I like to eat apples\" and \"I swim in the ocean.\"\\nQuestion answering\\nWriter’s AI is capable of answering questions based on information that\\'s been fed to our model. It usually involves taking information from a given text and using it to generate a response that is relevant to the question. For example, you may ask Writer’s AI to provide you with a plot summary of a book or ask why the sky is blue. \\nText classification\\n\\u200b\\u200bText classification is a process of assigning a class label to a piece of text. This is often done by first building a large language model, which is then used to generate labels for new pieces of text. For example, a language model might be used to label a piece of text as \"positive\" or \"negative\" based on the sentiment of the text.\\nZero-shot classification\\nZero-shot classification is a method of text classification that doesn\\'t require any training data. Instead, it relies on large language models that have been trained on a large amount of data. These models can learn the general structure of language and can therefore classify new text without any training data.\\nA relatable example of this would be if you were to take a large corpus of text, such as all of Wikipedia, and train a language model on it. This model would then be able to take any new text, such as a news article, and classify it according to its topic.\\nSummarization\\nSimply put, Writer’s AI is capable of summarizing any piece of text. For example, a reader wants to get the gist of a long article without reading the entire thing and would request a summary of this article. \\nText generation \\nLastly, what we all know generative AI to be, text generation is the act of producing new text from large language models. Emails, summaries, essays and such can all be generated as an example.\\nUpdated\\n \\nabout 2 months ago'}\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/hyperparameters>\n",
      "{'url': 'https://dev.writer.com/docs/hyperparameters', 'text': \"As a starting point, aim for at least a few hundred examples for fine-tuning  a model. Generally, the more data the better the model will adapt to a particular task. Additionally, tweaking the hyperparameters for fine-tuning can often lead to a model that produces higher quality completions. Hyperparameters are parameters that are used to control the learning process. \\nWe recommend using the default values for all hyperparameters:\\nBatch size\\n is the number of training samples (e.g., sentences, paragraphs) used to update the model's parameters during each iteration of training. In general, we've found that larger batch sizes tend to work better for larger datasets.\\nEpochs \\nrefers to the model being trained one full cycle through the dataset. The number of epochs you should set depends on the number of training examples in your dataset and the size of the model you're starting from. Generally, larger models and datasets (many thousands of training examples) will need fewer epochs to train. We recommend slightly overtraining your models. \\nLearning rate\\n is used to set the fine-tuning learning rate by determining how quickly the model updates the concepts it has learned.\\nChanging hyperparameters values from default may either increase or decrease the quality of the tuned model. Several rounds of experiments might be needed to find better models.\\nSmaller models tend to take more epochs to train.\\nModels can be badly trained for a number of reasons: a small dataset, large learning rates, or an insufficient number of epochs. In these cases, you might have to retrain the model with a different set of hyperparameters. \\nIn the future, we will allow logging the training process to W&B for inspection.\\nUpdated\\n \\n3 months ago\"}\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/toxic-check>\n",
      "{'url': 'https://dev.writer.com/docs/toxic-check', 'text': \"We developed an independent system that classifies input and output textual data and predicts its level of toxicity. Our model was trained on a distinct dataset that included both toxic and nontoxic examples. Each prompt is sent through the API to one of our large language models, and the generated text is analyzed and classified by a model that predicts the text's toxicity level. This probability lies between 0 and 1, where 0 indicates toxic classes and 1 indicates nontoxic classes.\\nThe text completion API methods return a JSON response with two fields, prompt labels and completion labels, that indicate the toxicity scores for each part independently. The field class name contains a binary class label determined by score, where a value less than 0.5 indicates toxicity and a value greater than 0.5 indicates nontoxicity.\\nUpdated\\n \\n4 months ago\"}\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/models> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/go-sdk> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/models>\n",
      "{'url': 'https://dev.writer.com/docs/models', 'text': \"The Writer LLM service enables you to customize and use the Writer LLMs outlined below.\\nThese large language models have been pre-trained on a massive amount of Internet text. Pre-training involves taking a mathematical model with random mathematical parameters (weights) and adjusting those weights iteratively in response to discrepancies between the model's output and a comparison point indicating the expected output. The most common training method for large language models is next-word prediction over massive amounts of text.\\nPalmyra Small is the fastest of Writer’s LLMs and can perform important tasks such as text parsing, simple classification, address correction, and keyword recognition. Providing more context drives better performance.\\nGood at: Text parsing, simple classification, address correction, and keyword recognition\\n\\n\\nPalmyra Base is extremely powerful as well as incredibly fast. This model excels at many nuanced tasks such as sentiment classification and summarization. Palmyra Base is also effective as a general service chatbot, answering questions and performing Q&A.\\nCompetent in: complex classification, text sentiment, and summarization\\n\\n\\nCamel-5b is a trained large language model that follows instructions. Based on Palmyra-Base is trained on ~70k instruction & response fine tuning records generated by Writer Team from the InstructGPT paper, including brainstorming, classification, closed quality assurance, generation, information extraction, open quality assurance, and summarization.\\nPalmyra Large is the most capable model family, capable of performing any task that the other models can, often with less instruction. Palmyra Large is good at comprehending the text's intent, solving logic problems, and explaining character motivations.\\nGood at: Few-shots, cause and effect, and audience summarization\\n\\n\\nInstructPalmyra is the most capable model. It can perform any tasks that the other models are able to, often with higher quality, longer output, and better instruction-following.\\nGood at: Zero-shots, cause and effect\\n\\n\\nPalmyra-R models are a general-purpose fine-tuning recipe for retrieval-augmented generation, combining pre-trained parametric and non-parametric memory for language generation. \\nIt is more capable than\\xa0the GPT-3\\xa0and GPT-3.5 models, able to perform more complex tasks, and comes in three flavors: General, Healthcare, and Fintech. It is available on-premise or via\\xa0an API.\\nSame capabilities as the Palmyra-E mode but with ~80K context length. still in early testing stage\\nUpdated\\n \\n19 days ago\"}\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/go-sdk>\n",
      "{'url': 'https://dev.writer.com/docs/go-sdk', 'text': 'Writer authenticates your API requests using your account’s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their team as a developer.\\nUpdated\\n \\n19 days ago'}\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/python-sdk> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/overview> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/usage-policies> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/python-sdk>\n",
      "{'url': 'https://dev.writer.com/docs/python-sdk', 'text': 'Writer authenticates your API requests using your account’s API keys. If you do not include your key when making an API request, or use one that is incorrect or outdated, Writer returns an error.\\nYour API keys are available in the account dashboard. We include randomly generated API keys in our code examples if you are not logged in. Replace these with your own or log in to see code examples populated with your own API keys.\\nIf you cannot see your secret API keys in the Dashboard, this means you do not have access to them. Contact your Writer account owner and ask to be added to their team as a developer.\\nUpdated\\n \\n19 days ago'}\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/overview>\n",
      "{'url': 'https://dev.writer.com/docs/overview', 'text': 'Libraries and tools for interacting with your Writer integration.\\nWriter’s server-side helper libraries (also known as Server-side SDKs) reduce the amount of work required to use Writer’s REST APIs, starting with reducing the boilerplate code you have to write. Below are the installation instructions for these libraries in a variety of popular server-side programming languages.\\nYou can access certain Writer products and features in the beta stage with beta SDKs. The versions of these beta SDKs have the beta or b suffix, for example, 1.0b3 in Python and 1.0-beta.3 in other language SDKs. Try these beta SDKs and share feedback with us before the features reach the stable phase. To learn more about how to use the beta SDKs, read the readme file in the GitHub repository of the individual language SDKs.\\nUpdated\\n \\nabout 2 months ago'}\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/page/security> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/integrations> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/integration-security-guide> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/usage-policies>\n",
      "{'url': 'https://dev.writer.com/docs/usage-policies', 'text': \"Please review the following policies to ensure responsible usage of our API: \\nContent Policy\\nThe Writer API doesn't allow for the generation of the following types of content: \\nPlatform Policy\\nFailure to comply with this policy may result in suspension or termination of access to the API.\\nWriter reserves the right to take necessary measures to enforce this policy and protect the system and other users from harm.\\nThis policy may be revised at any time without prior notice. Users are responsible for regularly checking the policy for updates.\\nUpdated\\n \\n2 months ago\"}\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/page/security>\n",
      "{'url': 'https://dev.writer.com/page/security', 'text': 'All security bugs in Writer are taken seriously and should be reported by emailing \\n[email\\xa0protected]\\n. This will be delivered to a subset of the core team who handle security issues.\\nYour email will be acknowledged within 24 hours, and you’ll receive a more detailed response to your email within 48 hours indicating the next steps in handling your report.\\nAfter the initial reply to your report, the security team will endeavor to keep you informed of the progress being made towards a fix and full announcement, and may ask for additional information or guidance surrounding the reported issue. These updates will be sent at least every five days, in practice, this is more likely to be every 24-48 hours.\\nSecurity bugs in third party systems should be reported to their respective maintainers and can also be coordinated through the Writer Security email.\\nHere is the security disclosure policy for Writer\\nThe security report is received and is assigned a primary handler. This person will coordinate the fix and release process. The problem is confirmed and a list of all affected versions is determined. Code is audited to find any potential similar problems. Fixes are prepared for all releases which are still under maintenance. These fixes are not committed to the public but rather held locally pending the announcement.\\nEvery effort will be made to handle the bug in as timely a manner as possible, however, it’s important that we follow the release process above to ensure that the disclosure is handled in a consistent manner.\\nIf you have suggestions on how this process could be improved please submit a pull request or email \\n[email\\xa0protected]\\n to discuss.'}\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/integrations>\n",
      "{'url': 'https://dev.writer.com/docs/integrations', 'text': 'Writer for Chrome\\nInstall Writer for Chrome to unlock on-brand content at scale. Improve your writing with line-by-line writing feedback, anywhere you use Chrome. Say goodbye to wordiness, over-formality, and all manner of writing errors.Stay consistent and on message with a custom, automated styleguide, use your latest terms, communicate in a healthy way, and be super efficient with autocorrect and autocomplete.\\nWriter for Figma\\nEverything you need to organize content design. Use the Writer + Figma plugin to make sure all your content is clear, consistent, and error-free. \\nWriter for Microsoft Word\\n \\nUse Writer to check your writing in Microsoft Word. This add-in takes your customized guidelines from Writer and applies it across your documents, in both desktop Word and Word 365.\\nWriter for Contentful \\nWriter is generative AI built for your business. The collaboration between Writer and Contentful enables marketers to make sure all of the content published through Contentful is clear, consistent, on-brand, and error-free.\\nFor help with installation, check out our help center \\nhere\\n.\\nUpdated\\n \\n3 months ago'}\n",
      "2023-05-03 15:30:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/rate-limits> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/integration-security-guide>\n",
      "{'url': 'https://dev.writer.com/docs/integration-security-guide', 'text': 'Ensure compliance and secure communications between Writer and your server.\\nTLS refers to the process of securely transmitting data between the client—the app or browser that your customer is using—and your server. This was originally performed using the SSL (Secure Sockets Layer) protocol. However, this is outdated and no longer secure, and has been replaced by TLS. The term “SSL” continues to be used colloquially when referring to TLS and its function to protect transmitted data.\\nPayment pages must make use of a modern version of TLS (e.g., TLS 1.2) as it significantly reduces the risk of you or your customers being exposed to a man-in-the-middle attack. TLS attempts to accomplish the following:\\nAdditionally, your customers are more comfortable sharing sensitive information on pages visibly served over HTTPS, which can help increase your customer conversion rate.\\nIf need be, you can test your integration without using HTTPS, and enable it once you’re ready to accept live charges. However, all interactions between your server and Writer must use TLS 1.2 (i.e, when using our libraries).\\nSERVING RESOURCES SECURELY\\n\\nYou should make sure that any resources (JavaScript, CSS, images, etc.) are also served over TLS to avoid a mixed content warning being shown to your customers in their browser.\\nA digital certificate—a file issued by a certification authority (CA)—is needed in order to use TLS. When installed, this certificate assures the client that it’s really communicating with the server it expects to be talking to, not an impostor. You should get a digital certificate from a reputable certificate provider, such as:\\nConceptually, setting up TLS is very straightforward: a certificate is purchased from a suitable provider, and then your server is configured to use it. The actual process does tend to be somewhat complex, and we recommend you follow the installation guide of the provider you use.\\nAs TLS is a complex suite of cryptographic tools, it’s easy to miss a few details. We recommend using the SSL Server Test by Qualys SSL Labs to make sure you have everything set up in a secure way.\\nIt can be a security risk to include JavaScript from other sites as your security becomes dependent on theirs. If they’re ever compromised, an attacker may be able to execute arbitrary code on your page. In practice, many sites make use of JavaScript for services like Google Analytics, even on secure pages. Nonetheless, it’s something to be aware of, and ideally minimize.\\nIf you’re making use of webhooks, we recommend using TLS for the endpoint to avoid traffic being intercepted and the notifications altered (sensitive information is never included in a webhook event).\\nWhile complying with the Data Security Standards is important, it shouldn’t be where you stop thinking about security. Some good resources to learn about web security are:\\nUpdated\\n \\n3 months ago'}\n",
      "2023-05-03 15:30:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/rate-limits>\n",
      "{'url': 'https://dev.writer.com/docs/rate-limits', 'text': \"What are rate limits?\\nA rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time. \\nNote: A request is defined as a single API call made by an API user. Requests can be made to any endpoint within the API.\\nWhy do we have rate limits?\\nWhat happens if I encounter a rate limit error? \\nIf you hit a rate limit, it means you've made too many requests in a short period of time, and the API is refusing to fulfill further requests until a specified amount of time has passed.\\nEach API key is allowed a certain number of requests per hour. If you exceed this limit, you will receive a 429 error response.\\nWhat are the rate limits for the Writer API? \\nHow can I check my rate limit status? \\nYou can check your current rate limit status at any moment using the developer dashboard.\\nTo check your current rate limit status, you can include the following headers in your request:\\nWhen should I consider requesting a rate limit increase? \\nOur default rate limits help us maintain stability and prevent abuse of our API. We increase limits to enable high-traffic applications, so the best time to apply for a rate limit increase is when you feel that you have the necessary traffic data to support a strong case for increasing the rate limit. \\nThis rate limiting policy may change from time to time. API users will be notified of any changes through the API documentation or by email.\\nWriter API calls of various categories are subject to differing rate constraints. Furthermore, the Enterprise API has its own set of restrictions.\\nUpdated\\n \\n3 months ago\"}\n",
      "2023-05-03 15:30:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/reference/contentcheck> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/errors> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/versioning> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/reference/contentcheck>\n",
      "{'url': 'https://dev.writer.com/reference/contentcheck', 'text': ''}\n",
      "2023-05-03 15:30:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/authentication> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dev.writer.com/docs/introduction> (referer: https://dev.writer.com/docs)\n",
      "2023-05-03 15:30:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/errors>\n",
      "{'url': 'https://dev.writer.com/docs/errors', 'text': 'Writer uses conventional HTTP response codes to indicate the success or failure of an API request. In general:\\nHere are some common error codes that can occur:\\nThese are just a few of the many error codes that can occur when making API requests. Understanding the meaning of these codes can help you diagnose and resolve issues when working with APIs.\\nUpdated\\n \\nabout 1 month ago'}\n",
      "2023-05-03 15:30:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/versioning>\n",
      "{'url': 'https://dev.writer.com/docs/versioning', 'text': 'The Writer API is versioned. The API version name is based on the date when the API version was released. For example, the current version is \\n2023-21-01\\n. \\nWhen backwards-incompatible changes are made to the API, a new, dated version is released. The current version is \\n2023-21-01.\\n Read our API upgrades guide to see our API changelog and to learn more about backwards compatibility.\\nThe changelog lists every available version. To set the API version on a specific request, send a Writer-Version header.\\nUpdated\\n \\nabout 1 month ago'}\n",
      "2023-05-03 15:30:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/authentication>\n",
      "{'url': 'https://dev.writer.com/docs/authentication', 'text': 'The Writer API uses API keys for authentication - please reach out to your admin to generate an API key for your requests. \\nDo not share your API keys\\n with others or expose it in any client-side code (browsers, apps). You should never, ever store these values in any publicly accessible location.  We recommend storing them as environment variables or accessed from a key management service. \\nAuthorization Type \\nWriter uses bearer authentication (also called token authentication), which is an HTTP authentication scheme that involves security tokens called bearer tokens. Bearer authentication (also called token authentication) is an \\nHTTP authentication scheme\\n that involves security tokens called bearer tokens. The name “Bearer authentication” can be understood as “give access to the bearer of this token.” The bearer token is a cryptic string, usually generated by the server in response to a login request. The client must send this token in the \\nAuthorization\\n header when making requests to protected resources:\\nThe Bearer authentication scheme was originally created as part of \\nOAuth 2.0\\n in \\nRFC 6750\\n, but is sometimes also used on its own. Similarly to \\nBasic authentication\\n, Bearer authentication should only be used over HTTPS (SSL).\\nUpdated\\n \\nabout 1 month ago'}\n",
      "2023-05-03 15:30:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dev.writer.com/docs/introduction>\n",
      "{'url': 'https://dev.writer.com/docs/introduction', 'text': 'The Writer API is organized around REST. Our API has predictable resource-oriented URLs, accepts form-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes, authentication, and verbs.\\nThe Writer API differs for every account as we release new versions and tailor functionality. Log in to see docs customized to your version of the API with your test key and data.\\nUpdated\\n \\nabout 1 month ago'}\n",
      "2023-05-03 15:30:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-05-03 15:30:35 [scrapy.extensions.feedexport] INFO: Stored json feed (30 items) in: site_text.json\n",
      "2023-05-03 15:30:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 9281,\n",
      " 'downloader/request_count': 35,\n",
      " 'downloader/request_method_count/GET': 35,\n",
      " 'downloader/response_bytes': 1408127,\n",
      " 'downloader/response_count': 35,\n",
      " 'downloader/response_status_count/200': 31,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'downloader/response_status_count/302': 3,\n",
      " 'dupefilter/filtered': 4,\n",
      " 'elapsed_time_seconds': 4.05155,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 5, 3, 22, 30, 35, 383757),\n",
      " 'httpcompression/response_bytes': 9426671,\n",
      " 'httpcompression/response_count': 31,\n",
      " 'item_scraped_count': 30,\n",
      " 'log_count/DEBUG': 67,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 4,\n",
      " 'memusage/max': 65470464,\n",
      " 'memusage/startup': 65470464,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 31,\n",
      " 'scheduler/dequeued': 35,\n",
      " 'scheduler/dequeued/memory': 35,\n",
      " 'scheduler/enqueued': 35,\n",
      " 'scheduler/enqueued/memory': 35,\n",
      " 'start_time': datetime.datetime(2023, 5, 3, 22, 30, 31, 332207)}\n",
      "2023-05-03 15:30:35 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider crawler/crawler/spiders/writer_spider.py -O site_text.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see exactly what data got gathered in `site_text.json`. If you look closely, you'll notice a few empty and duplicate pages. These could be filtered out with a couple of Scrapy rules, but they shouldn't affect query results so there's no harm in leaving them in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the scraping part is done, we can move on to using AI to query the information. The following code block is just to set up the Writer security object. Make sure you have a `.env` file in the parent directory with the following lines:\n",
    "```\n",
    "WRITER_ORG_ID=<your org ID>\n",
    "WRITER_API_KEY=<your API key>\n",
    "```\n",
    "\n",
    "or just directly set the corresponding variables in the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from writer import Writer\n",
    "from writer.models import shared\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"..\")\n",
    "org_id = os.environ.get(\"WRITER_ORG_ID\")\n",
    "api_key = os.environ.get(\"WRITER_API_KEY\")\n",
    "\n",
    "writer = Writer(\n",
    "    security=shared.Security(\n",
    "        api_key=api_key\n",
    "    ),\n",
    "    organization_id=org_id\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a function to split extracted text into chunks that can be compared to the input query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "CHUNK_TOKENS = 50\n",
    "\n",
    "def chunk_text(text: str):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    # We can tokenize the text to count the number of tokens for chunking\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    chunks = [\n",
    "        tokenizer.decode(tokens[i : i + CHUNK_TOKENS])\n",
    "        for i in range(0, len(tokens), CHUNK_TOKENS)\n",
    "    ]\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we need a function to actually compare those chunks. We use the `all-MiniLM-L6-v2` model from the `sentence_transformers` library for this. It checks the similarity in meaning between the input question and every chunk, and assigns a score to each one. Then we can rank them by this score to determine the most relevant pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heathexer/writer/writer-cookbook/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "TOP_N = 10\n",
    "\n",
    "def get_relevant_text(query: str, chunks: list[str]):\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    text_embeddings = model.encode(chunks)\n",
    "    query_embeddings = model.encode(query)\n",
    "\n",
    "    cos_sims = util.cos_sim(query_embeddings, text_embeddings)\n",
    "\n",
    "    sorted_chunk_nums = sorted(zip(cos_sims.tolist()[0], range(len(chunks))))\n",
    "    # We also take the chunk before and after each relevant chunk. \n",
    "    # Since the text is split arbitrarily, this is just in case some important context got cut off.\n",
    "    relevant_text = \"\\n\".join(\n",
    "        chunks[i - 1] or \"\" + chunks[i] + chunks[i + 1] or \"\"\n",
    "        for _, i in sorted_chunk_nums[-TOP_N:]\n",
    "    )\n",
    "\n",
    "    return relevant_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've sorted out a small, important subset of our contextual data, we can feed that along with our question to a generative language model to form into an actual answer. For this purpose, we are using Writer's `palmyra-instruct` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from writer.models import operations, shared\n",
    "\n",
    "def get_answer(query: str, text: str):\n",
    "    prompt = (\n",
    "        f\"Given the following context information, try to answer the following question with as much detail as possible.\"\n",
    "        f\"Use only the given context. Do not use outside information.\\n\"\n",
    "        f\"If no answer can be found in the given context, output \\\"Could not find an answer in your data.\\\"\\n\"\n",
    "        f\"Question: {query} \\n\\nContext:\\n{text}\\nAnswer: \"\n",
    "    )\n",
    "\n",
    "    req = operations.CreateCompletionRequest(\n",
    "        completion_request=shared.CompletionRequest(\n",
    "            prompt=prompt, max_tokens=2000, temperature=1.1\n",
    "        ),\n",
    "        model_id=\"palmyra-instruct\",\n",
    "    )\n",
    "\n",
    "    res = writer.completions.create(req)\n",
    "    if res.completion_response is not None:\n",
    "        return res.completion_response.choices[0].text\n",
    "    else:\n",
    "        print(res.fail_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can combine everything we've written into a simple function that takes in a question and returns an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def run_query(query: str):\n",
    "    site_data_file = open(\"site_text.json\", \"r\")\n",
    "    site_data = json.load(site_data_file)\n",
    "    chunks = []\n",
    "\n",
    "    for page in site_data:\n",
    "        chunks += chunk_text(page['text'])\n",
    "\n",
    "    relevant_text = get_relevant_text(query, chunks)\n",
    "\n",
    "    answer = get_answer(query, relevant_text)\n",
    "    return answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And ask it a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To use an API key, you will need to log into your Writer account dashboard and obtain the API keys. Once you have the keys, you should include them in your API request. If the correct keys are not used or if the keys have become outdated, Writer will return an error.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_query(\"How do I use an API key?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
